name: Ollama Python Integration

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch: # Allows manual triggering

jobs:
  ollama_inference_job:
    runs-on: ubuntu-latest # Using a standard Ubuntu runner

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Install Ollama
      run: |
        curl -fsSL https://ollama.com/install.sh | sh

    - name: Start Ollama server and pull model
      run: |
        ollama serve &

        echo "Waiting for Ollama server to be ready..."
        sleep 15 # Adjust this sleep duration if you face connection issues, especially for larger models

        curl http://localhost:11434 || { echo "Ollama server did not respond!"; exit 1; }
        echo "Ollama server is running."

        ollama pull llama3
        echo "Model llama3 pulled."

        # --- NEW: Run a test prompt directly here ---
        echo "Running a test prompt with Ollama..."
        ollama run llama3 "Why is the sky blue?" || { echo "Ollama test prompt failed!"; exit 1; }
        echo "Ollama test prompt completed successfully."
        # --- END NEW ---
      env:
        OLLAMA_HOST: "0.0.0.0"
        OLLAMA_KEEP_ALIVE: 5m

    - name: Set up Python environment
      uses: actions/setup-python@v5
      with:
        python-version: '3.x'

    - name: Install Python dependencies
      run: |
        pip install ollama

    - name: Run Python script to interact with Ollama
      run: python ollama_client.py
      env:
        OLLAMA_API_URL: http://localhost:11434
